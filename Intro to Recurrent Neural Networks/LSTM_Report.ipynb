{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Recurrent Neural Network to generate text\n",
    "<br>\n",
    "\n",
    "## Li Linwei\n",
    "\n",
    "<br>\n",
    "## Introduction<br>\n",
    "<font size=3>\n",
    "In this Implementation, I build an Recurrent Neural Network to train the text from an novel and use the trained model to generate a text with similar style using few chracters. <br>Since every character inside of a word has certain connected from each other, so I use LSTM network as hidden layer, they can save the state from previous character and based on the information from previous to predict what should be the next chracter. Some of the graphics are selected from Andrej Karpathy's post on RNNs.\n",
    "</font>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework<br>\n",
    "<font size=3>\n",
    "The following figure depicts the overall framework, if we have the word <font size=4>'hello'</font>, we will enter the characters sequentially: <br>\n",
    "<font size=4>'h => e => l => l', </font><font size=3>the targets are the inputs shifted over one character, so if the prediction is right, the result should be <br></font><font size=4>'e => l => l => o'</font> <font size=3>sequentially.</font><br><br>\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer\n",
    "<br>\n",
    "<font size=3>\n",
    "Since a text has thousands of word, every word has many characters, so we need to divide the text into batch, like the pictur below, and feed every batch to inputs at one time, so the </font><font size=4.5>Input layer</font> <font size=3>should be:\n",
    "<br><br>\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "<br><br><br>\n",
    "The details of the network can be illustrated as the figure below:</font><br><br>\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer ( LSTM )<br>\n",
    "<font size=3>I set the number of hidden layer as 2, that's a recommended number experimentally. Every LSTM layer has 512 hidden units, and I also use Dropout in the spatial output of every LSTM layer, the Dropout probablity is 0.5.</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Output Layer<br>\n",
    "<font size=3>The </font> <font size=4.5>output layer</font> <font size=4.5> uses softmax as the activation function, which can provided us a sequence of probability distribution, the network can determine the most probable character according to this distribution and predict the next character, so the number of output should be the number of characters which ever existed in total text, include some characters like blank or punctuation. <br></font>\n",
    "\n",
    "## Loss<br>\n",
    "<font size=3>The </font> <font size=4.5>loss</font> <font size=3>of output is calculated by cross-entropy of prediction sequence and target sequence. We also add a gradients_clip as threshold in order to prevent gradients explode during gradients descent, That is, if a gradient is larger than that threshold, we set it to the threshold.\n",
    "</font>\n",
    "\n",
    "\n",
    "## Optimizer<br>\n",
    "<font size=3> And as for the optimizer we use </font> <font size=4.5> Adam Optimizer</font>,<font size=3> which has been recommended in many place.</font>\n",
    "<br>\n",
    "## Train\n",
    "<font size=3>\n",
    "The Hyperparameters are set as follow:\n",
    "</font>\n",
    "```python\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "```\n",
    "<br>\n",
    "<font size=3>\n",
    "The </font><font size=4.5>last episode</font><font size=3> of train results are shown below:\n",
    "```python\n",
    "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.0423...  2.5772 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.0606...  2.5905 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.0346...  2.6036 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.0432...  2.6674 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.0548...  2.6038 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.0414...  2.5892 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.0342...  2.5783 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.0558...  2.6327 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.0478...  2.6311 sec/batch\n",
    "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.0426...  2.5564 sec/batch\n",
    "```\n",
    "<font size=4.5>Training Step</font> <font size=3>are the number of batches,</font> <font size=4.5>sec/batch</font> <font size=3>is speed, the</font> <font size=4.5>Training loss</font><font size=3>  is 1.0426 finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "<br>\n",
    "<font size=3>\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text.\n",
    "<br></font>\n",
    "### For example:\n",
    "```python\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)\n",
    "```\n",
    "### Episode of generated text ==> \n",
    "```\n",
    "\"Yes, I shall see that!\"\n",
    "\n",
    "And all sildey stopped, was futtle and helpless scruminy, and he stopped,\n",
    "she was terrible, and he was so far from the door, and so sheally careed\n",
    "for her son stumble that he was not enjoying it to see them in the\n",
    "sacromman with her son, who could not be sent to all her face;\n",
    "business of this thoughts at any other in the carriage, that had\n",
    "sent to him the general work of her, then it will happen of the sound,\n",
    "though it all of to the serfate sudden, and he went into the doctor, which he\n",
    "would fing that infered the most intense, thoughts were te get a ballstake,\n",
    "and he started still, looked at him, stood still of two creature,\n",
    "which she felt the servant, and at those whise he had left to him that he\n",
    "would stay over. The prince, and said that they all all done was\n",
    "things if it had triet, because he child not take another that way.\n",
    "\n",
    "\"That is the hutal prince, because she dod't a came on all the world when\n",
    "I think of that in the country is applaced to her in the cuttom.\"\n",
    "\n",
    "\"And that's just at once into the steps. It well time the same way,\n",
    "talkly, and we've not the sacriment of their position,\" he said,\n",
    "truncting himself in the same; \"I shall be definite it, and never can he\n",
    "got on living mestal at alter, and I did my everything--tell me\n",
    "her husband, that I should go and destroyed.\"\n",
    "```\n",
    "<br>\n",
    "## Conclusion\n",
    "<font size=3>\n",
    "That's the text generated by Input chracters \"Far\". As we can see almost every word are grammatically correct, the using of punctuation is also correct, the style of the text should be similar to the original novel, only some of the sentences are hard to undestand.<br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Code of the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from, shape=(1985223,)\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps # 一个batch的总字符数\n",
    "    n_batches = len(arr)//batch_size # batch的个数\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size] # 舍弃一些组不成一个batch的字符\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1)) \n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size,num_steps), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size,num_steps), name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability 用来给LSTMcell加Dropout\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells 用来初始化LSTM输出的dimension\n",
    "        num_layers: Number of LSTM layers 用来堆叠多层LSTM网络\n",
    "        batch_size: Batch size 用来生成initial_state, 此例中详见下图\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell outputs\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop for _ in range(num_layers)])\n",
    "    \n",
    "    # batch_size: 一个batch的总字符数\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32) \n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        # LSTM cell的个数, 有几个cell就代表有几层, 至于横向时间展开是为了方便理解, 实际并不存在\n",
    "        out_size: Size of this softmax layer\n",
    "        # softmax layer应该和text中出现的字母种类数保持一致, 返回一个概率list, 每个位置代表相应的概率\n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    # reshape中的-1表示推断, 即给出其中一个维度参数的情况下, 推断出另一个维度应该是多少\n",
    "    # in_size表示LSTM cell的个数, 这一步的输出应该是每层cell的输出列成一列\n",
    "    x = tf.reshape(seq_output, [-1,in_size]) \n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size,out_size], stddev=0.1, dtype=tf.float32))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='prediction')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, shape=(logits.shape))\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    \n",
    "    # 只要Variable变量的 trainable = True(默认为True), 则tf.trainable_variables()会收集所有Variable类的实例(变量).\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "            \n",
    "        # Reset graph    \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #build input placeholder\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps) \n",
    "        \n",
    "        # Build the LSTM cell\n",
    "        self.lstm, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        ### Run the data through the RNN layer\n",
    "        # one-hot encode the input token\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through RNN with tf.nn.dynamic_run\n",
    "        outputs, state = tf.nn.dynamic_rnn(self.lstm, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax prediction and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Train the network ######\n",
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "#     saver.restore(sess, 'checkpoints/i78200_l128.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
