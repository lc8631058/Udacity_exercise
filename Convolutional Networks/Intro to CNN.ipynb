{
 "cells": [
  {
   "attachments": {
    "neilsen-pic.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking up an Image\n",
    "<font size =3>\n",
    "The first step for a CNN is to break up the image into smaller pieces. We do this by selecting a width and height that defines a filter.\n",
    "\n",
    "The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter.<br>\n",
    "<img src=\"vlcsnap-2016-11-24-15h52m47s438.png\" width=\"55%\">\n",
    "<br>\n",
    "We then simply slide this filter horizontally or vertically to focus on a different piece of the image.\n",
    "\n",
    "The amount by which the filter slides is referred to as the 'stride'. The stride is a hyperparameter which you, the engineer, can tune. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\n",
    "\n",
    "Let’s look at an example. In this zoomed in image of the dog, we first start with the patch outlined in red. The width and height of our filter define the size of this square.\n",
    "\n",
    "</font>\n",
    "# Filter Depth\n",
    "<font size=3>\n",
    "It's common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a convolutional layer is called the ***filter depth***.\n",
    "![neilsen-pic.png](attachment:neilsen-pic.png)\n",
    "How many neurons does each patch connect to?\n",
    "\n",
    "That’s dependent on our filter depth. If we have a depth of k, we connect each patch of pixels to k neurons in the next layer. This gives us the height of k in the next layer, as shown below. In practice, k is a hyperparameter we tune, and most CNNs tend to pick the same starting values.\n",
    "<img src=\"filter-depth.png\" width=\"200\">\n",
    "But why connect a single patch to multiple neurons in the next layer? Isn’t one neuron good enough?\n",
    "\n",
    "Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture.\n",
    "\n",
    "For example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue. In that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue.\n",
    "<img src=\"teeth-whiskers-tongue.png\" width=\"200\">\n",
    "Having multiple neurons for a given patch ensures that our CNN can learn to capture whatever characteristics the CNN learns are important.\n",
    "\n",
    "Remember that the CNN isn't \"programmed\" to look for certain characteristics. Rather, it learns on its own which characteristics to notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<img src=\"vlcsnap-2016-11-24-16h01m35s262.png\" width=\"500\">\n",
    "When we are trying to classify a picture of a cat, we don’t care where in the image a cat is. If it’s in the top left or the bottom right, it’s still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this?\n",
    "\n",
    "As we saw earlier, the classification of a given patch in an image is determined by the weights and biases corresponding to that patch.\n",
    "\n",
    "If we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way.\n",
    "\n",
    "This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "\n",
    "There’s an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Padding\n",
    "<font size=3>\n",
    "<img src=\"screen-shot-2016-11-24-at-10.05.37-pm.png\" width=\"200\">\n",
    "Let's say we have a 5x5 grid (as shown above) and a filter of size 3x3 with a stride of 1. What's the width and height of the next layer? We see that we can fit at most three patches in each direction, giving us a dimension of 3x3 in our next layer. As we can see, the width and height of each subsequent layer decreases in such a scheme.\n",
    "\n",
    "In an ideal world, we'd be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simply add a border of 0s to our original 5x5 image. You can see what this looks like in the below image.\n",
    "<img src=\"screen-shot-2016-11-24-at-10.05.46-pm.png\" width=\"200\">\n",
    "This would expand our original image to a 7x7. With this, we now see how our next layer's size is again a 5x5, keeping our dimensionality consistent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality\n",
    "<font size=3>\n",
    "From what we've learned so far, how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "Given:\n",
    "\n",
    "our input layer has a width of W and a height of H\n",
    "our convolutional layer has a filter size F\n",
    "we have a stride of S\n",
    "a padding of P\n",
    "and the number of filters K,\n",
    "the following formula gives us the width of the next layer: \n",
    "```python\n",
    "W_out = (W−F+2P)/S+1.\n",
    "```\n",
    "\n",
    "The output height would be \n",
    "```python\n",
    "H_out = (H-F+2P)/S + 1.\n",
    "```\n",
    "And the output depth would be equal to the number of filters \n",
    "```python\n",
    "D_out = K.\n",
    "```\n",
    "The output volume would be \n",
    "```python\n",
    "W_out * H_out * D_out.\n",
    "```\n",
    "Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz:  Convolution Output Shape\n",
    "\n",
    "For the next few quizzes we'll test your understanding of the dimensions in CNNs. Understanding dimensions will help you make accurate tradeoffs between model size and performance. As you'll see, some parameters have a much bigger impact on model size than others.\n",
    "\n",
    "Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "We have an input of shape 32x32x3 (HxWxD)\n",
    "20 filters of shape 8x8x3 (HxWxD)\n",
    "A stride of 2 for both the height and width (S)\n",
    "With padding of size 1 (P)\n",
    "Recall the formula for calculating the new height or width:\n",
    "\n",
    "## Solution\n",
    "\n",
    "The answer is **14x14x20**.\n",
    "\n",
    "We can get the new height and width with the formula resulting in:\n",
    "```python\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "```\n",
    "The new depth is equal to the number of filters, which is 20.\n",
    "<br>This would correspond to the following code:\n",
    "```python\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "padding = 'SAME'\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "```\n",
    "Note the output shape of conv will be [1, 16, 16, 20]. It's 4D to account for batch size, but more importantly, it's not [1, 14, 14, 20]. This is because the padding algorithm TensorFlow uses is not exactly the same as the one above. An alternative algorithm is to switch padding from 'SAME' to 'VALID' which would result in an output shape of [1, 13, 13, 20]. If you're curious how padding works in TensorFlow, read this [document: how padding works in tf.nn.conv2d( )](https://www.tensorflow.org/api_guides/python/nn#Convolution).\n",
    "\n",
    "In summary TensorFlow uses the following equation for 'SAME' vs 'PADDING'<br>\n",
    "<br>\n",
    "\n",
    "**SAME Padding**, the output height and width are computed as:\n",
    "```python\n",
    "out_height = ceil(float(in_height) / float(strides[1]))\n",
    "\n",
    "out_width = ceil(float(in_width) / float(strides[2]))\n",
    "```\n",
    "**VALID Padding**, the output height and width are computed as:\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "\n",
    "out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n",
    "### [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "<br>\n",
    "### [explain of tf.nn.conv2d in stackoverflow](http://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow)\n",
    "<br>\n",
    "Your example is 1 image, size 2x2, with 1 channel. You have 1 filter, with size 1x1, and 1 channel (size is height x width x channels x number of filters).\n",
    "\n",
    "For this simple case the resulting 2x2, 1 channel image (size 1x2x2x1, number of images x height x width x x channels) is the result of multiplying the filter value by each pixel of the image.\n",
    "\n",
    "Now let's try more channels:\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([1,3,3,5]))\n",
    "filter = tf.Variable(tf.random_normal([1,1,5,1]))\n",
    "```\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "Here the 3x3 image and the 1x1 filter each have 5 channels. The resulting image will be 3x3 with 1 channel (size 1x3x3x1), where the value of each pixel is the dot product across channels of the filter with the corresponding pixel in the input image.\n",
    "\n",
    "Now with a 3x3 filter\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([1,3,3,5]))\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,1]))\n",
    "```\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "Here we get a 1x1 image, with 1 channel (size 1x1x1x1). The value is the sum of the 9, 5-element dot products. But you could just call this a 45-element dot product.\n",
    "\n",
    "Now with a bigger image\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,1]))\n",
    "```\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "The output is a 3x3 1-channel image (size 1x3x3x1). Each of these values is a sum of 9, 5-element dot products.\n",
    "\n",
    "Each output is made by centering the filter on one of the 9 center pixels of the input image, so that none of the filter sticks out. The xs below represent the filter centers for each output pixel.\n",
    "```python\n",
    ".....\n",
    ".xxx.\n",
    ".xxx.\n",
    ".xxx.\n",
    ".....\n",
    "```\n",
    "Now with \"SAME\" padding:\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,1]))\n",
    "\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```\n",
    "This gives a 5x5 output image (size 1x5x5x1). This is done by centering the filter at each position on the image.\n",
    "\n",
    "Any of the 5-element dot products where the filter sticks out past the edge of the image get a value of zero.\n",
    "\n",
    "So the corners are only sums of 4, 5-element dot products.\n",
    "\n",
    "Now with multiple filters.\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,7]))\n",
    "\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```\n",
    "This still gives a 5x5 output image, but with 7 channels (size 1x5x5x7). Where each channel is produced by one of the filters in the set.\n",
    "\n",
    "Now with strides 2,2:\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,7]))\n",
    "\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding='SAME')\n",
    "```\n",
    "Now the result still has 7 channels, but is only 3x3 (size 1x3x3x7).\n",
    "\n",
    "This is because instead of centering the filters at every point on the image, the filters are centered at every other point on the image, taking steps (strides) of width 2. The x's below represent the filter center for each output pixel, on the input image.\n",
    "```python\n",
    "x.x.x\n",
    ".....\n",
    "x.x.x\n",
    ".....\n",
    "x.x.x\n",
    "```\n",
    "And of course the first dimension of the input is the number of images so you can apply it over a batch of 10 images, for example:\n",
    "```python\n",
    "input = tf.Variable(tf.random_normal([10,5,5,5]))\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,7]))\n",
    "\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding='SAME')\n",
    "```\n",
    "This performs the same operation, for each image independently, giving a stack of 10 images as the result (size 10x3x3x7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Number of Parameters\n",
    "We're now going to calculate the number of parameters of the convolutional layer. The answer from the last quiz will come into play here!\n",
    "\n",
    "Being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses.\n",
    "\n",
    "## Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "We have an input of shape 32x32x3 (HxWxD)\n",
    "20 filters of shape 8x8x3 (HxWxD)\n",
    "A stride of 2 for both the height and width (S)\n",
    "Zero padding of size 1 (P)\n",
    "## Output Layer\n",
    "14x14x20 (HxWxD)\n",
    "## Hint\n",
    "Without parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron.\n",
    "## Solution\n",
    "There are 756560 total parameters. That's a HUGE amount! Here's how we calculate it:\n",
    "\n",
    "(8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560\n",
    "\n",
    "8 * 8 * 3 is the number of weights, we add 1 for the bias. Remember, each weight is assigned to every single part of the output (14 * 14 * 20). So we multiply these two numbers together and we get the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Parameter Sharing\n",
    "Now we'd like you to calculate the number of parameters in the convolutional layer, if every neuron in the output layer shares its parameters with every other neuron in its same channel.\n",
    "\n",
    "This is the number of parameters actually used in a convolution layer (tf.nn.conv2d()).\n",
    "\n",
    "## Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "We have an input of shape 32x32x3 (HxWxD)\n",
    "20 filters of shape 8x8x3 (HxWxD)\n",
    "A stride of 2 for both the height and width (S)\n",
    "Zero padding of size 1 (P)\n",
    "## Output Layer\n",
    "14x14x20 (HxWxD)\n",
    "## Hint\n",
    "With parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel. So the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer.\n",
    "## Solution\n",
    "There are 3860 total parameters. That's 196 times fewer parameters! Here's how the answer is calculated:\n",
    "\n",
    "(8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860\n",
    "\n",
    "That's 3840 weights and 20 biases. This should look similar to the answer from the previous quiz. The difference being it's just 20 instead of (14 * 14 * 20). Remember, with weight sharing we use the same filter for an entire depth slice. Because of this we can get rid of 14 * 14 and be left with only 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNNs\n",
    "<font size=3>\n",
    "Let’s look at an example CNN to see how it works in action.\n",
    "\n",
    "The CNN we will look at is trained on ImageNet as described in [this paper](http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf) by Zeiler and Fergus. In the images below (from the same paper), we’ll see what each layer in this network detects and see how each layer detects more and more complex ideas.\n",
    "</font>\n",
    "## Layer 1\n",
    "<img src=\"layer-1-grid.png\" width=\"120\">\n",
    "<font size=3>\n",
    "Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle).\n",
    "\n",
    "The images above are from Matthew Zeiler and Rob Fergus' [deep visualization toolbox](https://www.youtube.com/watch?v=ghEmQSxT6tw), which lets us visualize what each layer in a CNN focuses on.\n",
    "\n",
    "Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference.\n",
    "<img src=\"diagonal-line-1.png\" width=\"70\">\n",
    "As visualized here, the first layer of the CNN can recognize -45 degree lines.\n",
    "<img src=\"diagonal-line-2.png\" width=\"70\">\n",
    "The first layer of the CNN is also able to recognize +45 degree lines, like the one above.\n",
    "\n",
    "Let's now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.\n",
    "\n",
    "<img src=\"grid-layer-1.png\" width=\"120\">\n",
    "\n",
    "Example patches that activate the -45 degree line detector in the first layer.\n",
    "So, the first layer of our CNN clearly picks out very simple shapes and patterns like **lines** and **blobs**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2\n",
    "\n",
    "<img src=\"screen-shot-2016-11-24-at-12.09.02-pm.png\" width=\"700\"><font size=3>\n",
    "A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.\n",
    "The second layer of the CNN captures complex ideas.\n",
    "\n",
    "As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right).\n",
    "\n",
    "\n",
    "The CNN learns to do this on its own. There is no special instruction for the CNN to focus on more complex objects in deeper layers. That's just how it normally works out when you feed training data into a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 3\n",
    "<img src=\"screen-shot-2016-11-24-at-12.09.24-pm.png\" width=\"700\">\n",
    "<font size=3>\n",
    "\n",
    "The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 5\n",
    "<img src=\"screen-shot-2016-11-24-at-12.08.11-pm.png\" width=\"700\">\n",
    "<font size=3>\n",
    "<br>\n",
    "We'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN.\n",
    "\n",
    "The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles.\n",
    "</font>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# TensorFlow Convolution Layer\n",
    "<font size=3>\n",
    "Let's examine how to implement a CNN in TensorFlow.\n",
    "\n",
    "TensorFlow provides the [tf.nn.conv2d()](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) and [tf.nn.bias_add()](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add) functions to create your own convolutional layers.\n",
    "<br>\n",
    "</font>\n",
    "```python\n",
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "# Weight的size应该和filter的size是一样的\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "```\n",
    "<font size=3>\n",
    "<br>\n",
    "The code above uses the **tf.nn.conv2d()** function to compute the convolution with weight as the filter and [1, 2, 2, 1] for the strides. TensorFlow uses a stride for each **input** dimension, **[batch, input_height, input_width, input_channels]**. We are generally always going to set the stride for **batch** and **input_channels** (i.e. the first and fourth element in the strides array) to be 1.\n",
    "<br><br>\n",
    "You'll focus on changing **input_height** and **input_width** while setting batch and input_channels to 1. The input_height and input_width strides are for striding the filter over input. This example code uses a stride of 2 with 5x5 filter over input.\n",
    "<br><br>\n",
    "The **tf.nn.bias_add()** function adds a 1-d bias to the last dimension in a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Max Pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"max-pooling.png\" width=\"500\">\n",
    "<font size=3>\n",
    "The image above is an example of max pooling with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find the maximum value.<br>\n",
    "\n",
    "For example, [[1, 0], [4, 6]] becomes 6, because 6 is the maximum value in this set. Similarly, [[2, 3], [6, 8]] becomes 8.<br>\n",
    "\n",
    "Conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values.<br>\n",
    "\n",
    "TensorFlow provides the [tf.nn.max_pool( )](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function to apply max pooling to your convolutional layers.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The tf.nn.max_pool() function performs max pooling with the **ksize** parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.<br>\n",
    "\n",
    "The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor **([batch, height, width, channels])**. ***For both ksize and strides, the batch and channel dimensions are typically set to 1***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Recently, pooling layers have fallen out of favor. Some reasons are: \n",
    "（pooling越来越不受欢迎的原因）\n",
    "\n",
    "1.Recent datasets are so big and complex we're more concerned about underfitting.<br>\n",
    "2.Dropout is a much better regularizer.<br>\n",
    "3.Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network in TensorFlow\n",
    "<font size=3>\n",
    "It's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow.\n",
    "\n",
    "The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.<br>\n",
    "<br>\n",
    "Just like in that segment, here you'll study the line-by-line breakdown of the code. If you want, you can even download the code and run it yourself.\n",
    "\n",
    "Thanks to [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples) for providing the original TensorFlow model on which this segment is based.\n",
    "\n",
    "Time to dive in!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "<font size=3>\n",
    "Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "<img src=\"convolution-schematic.gif\" width=\"500\">\n",
    "<font size=3>The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, [[1, 0, 1], [0, 1, 0], [1, 0, 1]], then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using tf.nn.conv2d() and tf.nn.bias_add().\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    The tf.nn.conv2d() function computes the convolution against weight W as shown above.\n",
    "\n",
    "In TensorFlow, strides is an array of 4 elements; the first element in this array indicates the stride for batch and last element indicates stride for features. It's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them. You can always set the first and last element to 1 in strides in order to use all batches and features.\n",
    "\n",
    "The middle two elements are the strides for height and width respectively. I've mentioned stride as one number because you usually have a square stride where height = width. When someone says they are using a stride of 3, they usually mean tf.nn.conv2d(x, W, strides=[1, 3, 3, 1]).\n",
    "\n",
    "To make life easier, the code is using tf.nn.bias_add() to add the bias. Using tf.add() doesn't work when the tensors aren't the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =3>\n",
    "The [tf.nn.max_pool( )](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function does exactly what you would expect, it performs max pooling with the ksize parameter as the size of the filter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "<img src=\"arch.png\" width=\"500\">\n",
    "<font size=3>\n",
    "In the code below, we're creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from conv1 to output, producing 10 class predictions.\n",
    "</font>\n",
    "<font size=5>\n",
    "To get a tensor's shape as a list of ints, use do **tensor.get_shape().as_list()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 -Loss: 55823.6406 Validation Accuracy: 0.050781\n",
      "Epoch  1, Batch   2 -Loss: 53121.8984 Validation Accuracy: 0.066406\n",
      "Epoch  1, Batch   3 -Loss: 43087.0547 Validation Accuracy: 0.050781\n",
      "Epoch  1, Batch   4 -Loss: 35085.2969 Validation Accuracy: 0.070312\n",
      "Epoch  1, Batch   5 -Loss: 27708.0527 Validation Accuracy: 0.093750\n",
      "Epoch  1, Batch   6 -Loss: 26328.6875 Validation Accuracy: 0.109375\n",
      "Epoch  1, Batch   7 -Loss: 22268.6992 Validation Accuracy: 0.109375\n",
      "Epoch  1, Batch   8 -Loss: 20284.0938 Validation Accuracy: 0.117188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a0a709d70f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_valid_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_valid_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 keep_prob: 1.})\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             print('Epoch {:>2}, Batch {:>3} -'\n",
      "\u001b[0;32m/Users/congcong/anaconda/envs/tensorflow1.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/envs/tensorflow1.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/envs/tensorflow1.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/congcong/anaconda/envs/tensorflow1.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/envs/tensorflow1.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
