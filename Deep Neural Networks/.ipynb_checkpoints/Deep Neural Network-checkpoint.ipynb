{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![two-layer-network.png](attachment:two-layer-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks\n",
    "<font size=3>\n",
    "In this lesson, you'll learn how to build multilayer neural networks with TensorFlow. Adding a hidden layer to a network allows it to model more complex functions. Also, using a non-linear activation function on the hidden layer lets it model non-linear functions.\n",
    "\n",
    "We shall learn about ReLU, a non-linear function, or rectified linear unit. The ReLU function is 0 for negative inputs and x for all inputs x>0.<br>\n",
    "<br>\n",
    "Next, you'll see how a ReLU hidden layer is implemented in TensorFlow.</font>\n",
    "<br>\n",
    "\n",
    "### [1. TensorFlow ReLUs](#lesson_1)\n",
    "### [2. Deep Neural Network in TensorFlow](#lesson_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow ReLUs<a id='lesson_1'></a>\n",
    "<font size=3>\n",
    "TensorFlow provides the ReLU function as [**tf.nn.relu()**](https://www.tensorflow.org/api_docs/python/tf/nn/relu), as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "The above code applies the **tf.nn.relu( )** function to the **hidden_layer**, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "<font size=3>\n",
    "Below you'll use the ReLU function to turn a linear single layer network into a non-linear multilayer network.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "output_layer = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "    \n",
    "# TODO: Create Model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "# TODO: Print session results  \n",
    "    print(sess.run(output_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network in TensorFlow<a id='lesson_2'></a>\n",
    "<font size=3>\n",
    "You've seen how to build a logistic classifier using TensorFlow. Now you're going to see how to use the logistic classifier to build a deep neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step\n",
    "<font size=3>\n",
    "In the following walkthrough, we'll step through TensorFlow code written to classify the letters in the MNIST database. If you would like to run the network on your computer, the file is provided in **multilayer_perceptron.py**. You can find this and many more examples of TensorFlow at [**Aymeric Damien's GitHub repository**](https://github.com/aymericdamien/TensorFlow-Examples).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "## TensorFlow MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=3>You'll use the MNIST dataset provided by TensorFlow, which batches and One-Hot encodes the data for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parameters\n",
    "<br>\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "```\n",
    "\n",
    "<font size=3>\n",
    "The focus here is on the architecture of multilayer neural networks, not parameter tuning, so here we'll just give you the learning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer Parameters<br>\n",
    "```python\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "```\n",
    "\n",
    "<font size=3>\n",
    "The variable n_hidden_layer determines the size of the hidden layer in the neural network. This is also known as the width of a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "```python\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}```\n",
    "\n",
    "<font size=3>\n",
    "Deep neural networks use multiple layers with each layer requiring it's own weight and bias. The **'hidden_layer'** weight and bias is for the hidden layer. The **'out'** weight and bias is for the output layer. If the neural network were deeper, there would be weights and biases for each additional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "```python\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "```\n",
    "<font size=3>\n",
    "The MNIST data is made up of 28px by 28px images with a single channel. The [**tf.reshape( )**](https://www.tensorflow.org/api_docs/python/tf/reshape) function above reshapes the 28px by 28px matrices in x into row vectors of 784px.<br>\n",
    "<br>\n",
    "Examples:\n",
    "</font>\n",
    "\n",
    "```python\n",
    "# tensor 't' is [[[1, 1, 1],\n",
    "#                 [2, 2, 2]],\n",
    "#                [[3, 3, 3],\n",
    "#                 [4, 4, 4]],\n",
    "#                [[5, 5, 5],\n",
    "#                 [6, 6, 6]]]\n",
    "# tensor 't' has shape [3, 2, 3]\n",
    "# pass '[-1]' to flatten 't'\n",
    "reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n",
    "\n",
    "# -1 can also be used to infer the shape\n",
    "\n",
    "# -1 is inferred to be 9:\n",
    "reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
    "                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
    "# -1 is inferred to be 2:\n",
    "reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
    "                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
    "# -1 is inferred to be 3:\n",
    "reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n",
    "                              [2, 2, 2],\n",
    "                              [3, 3, 3]],\n",
    "                             [[4, 4, 4],\n",
    "                              [5, 5, 5],\n",
    "                              [6, 6, 6]]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "![multi-layer.png](attachment:multi-layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n",
    "    biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
